{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Entity Resolution with DGL on Amazon SageMaker\n",
    "\n",
    "This notebook how to train an entity resolution model using graph neural networks. Entity resolution is the task of identifying and linking entites in a graph that belong to the same real world entity. This is useful for use-cases like user profiling where users might access an online service via different temporary session IDs generated by different devices. Entity resolution allows us to consolidate all information about a particular user and deduplicate the user profile.\n",
    "\n",
    "There are two main parts of this notebook.\n",
    "* First, we process the raw dataset to prepare the features and construct the graph.\n",
    "* Next, we create a launch a training job using the SageMaker to train a graph neural network model with DGL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash setup.sh\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker_graph_entity_resolution import config\n",
    "\n",
    "role = config.role\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload raw data to S3\n",
    "\n",
    "The dataset we use is the [DCA dataset](https://drive.google.com/drive/folders/0B7XZSACQf0KdNXVIUXEyVGlBZnc?usp=drive_open) released as part of the [2016 CIKM Cup competition](https://competitions.codalab.org/competitions/11171). The dataset contains anonymized browsing logs of user accessing various urls. In order to ensure the demonstration runs quickly, and to match the typical format of user activity data that many companies have, we have done some initial preparation steps. We converted the data from json to a relational table format and sampled just a subset of the overall data. The data preparation scripts can be seen in the `data-prep/` folder\n",
    "\n",
    "The prepared dataset consists of two files:\n",
    "\n",
    "* `logs.csv`: Records user browsing activity. Each entry consists of a timestamp, the anonymized urls that the user visited, the anonymized title of the url page, and the anonymized transient user id. The column names for the dataset are `['ts', 'urls', 'titles', 'uid']`\n",
    "\n",
    "* `train.csv`: Records ground truth links between pairs of transient user ids. Each entry has a pair of uids that are known to be the same real world user. The file has no headers.\n",
    "\n",
    "\n",
    "Now, let's move the raw data to a convenient location in an S3 bucket in your account for this proejct. There it will be picked up by the preprocessing job and training job.\n",
    "\n",
    "If you would like to use your own dataset for this demonstration. Replace the `raw_data_location` in the cell below with the s3 path or local path of your dataset, and modify the data preprocessing step as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with an S3 location or local path to point to your own dataset\n",
    "raw_data_location = 's3://{}/{}/data'.format(config.solution_upstream_bucket, config.solution_name)\n",
    "\n",
    "session_prefix = 'dgl-entity-resolution'\n",
    "input_data = 's3://{}/{}/{}'.format(config.solution_bucket, session_prefix, config.s3_data_prefix)\n",
    "\n",
    "!aws s3 cp --recursive $raw_data_location $input_data\n",
    "\n",
    "# Set S3 locations to store processed data for training and post-training results and artifacts respectively\n",
    "train_data = 's3://{}/{}/{}'.format(config.solution_bucket, session_prefix, config.s3_processing_output)\n",
    "train_output = 's3://{}/{}/{}'.format(config.solution_bucket, session_prefix, config.s3_train_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Preprocessing job with Amazon SageMaker Processing\n",
    "\n",
    "The script we have defined at `data-preprocessing/data_preprocessing.py` performs data preprocessing and feature engineering transformations on the raw tabular data.\n",
    "\n",
    "We convert the relational table to graph edgelists describing the relationships. For example the columns `['uid', 'urls']` are converted to an edgelist for edge type `('user', 'visits', 'url')` and the columns `['urls', 'titles']` are converted into an edgelist for edge type `('url', 'owned_by', 'domain')`.\n",
    "\n",
    "\n",
    "We also perform feature engineering to generate features for each user and each domain.\n",
    "\n",
    "* User features: We use the timestamps in the `ts` column to generate k-hot feature vectors that encode users' weekly browsing habits. For each user, we generate a 168 dimensional (7 days * 24 hours) vector.\n",
    "\n",
    "* Url features: We use the anonymized tokens in the full url and title to generate features for the url. For example if a url is `a/b/c?d` and the title is `a`, we have a bag of words of `['a', 'b', 'c', 'd']` for the url.  We use a [TfIdfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) to convert the text features into numerical features and then perform [dimensionality reduction](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html) to obtain a 20 dimensional feature vector.\n",
    "\n",
    "In order to adapt the preprocessing script to work with your data in the same format, you can modify the python script `data-preprocessing/data_preprocessing_script.py` used in the cell below.\n",
    "\n",
    "The python processing script also splits our ground-truth linked entities into a train and test/validation set. The default test-ratio is 0.3 but this can be modified.\n",
    "\n",
    "We use the built SKLearnProcessor provided SageMaker since it already contains the python dependencies - (pandas, sklearn) - that we need for preprocessing and feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(framework_version='0.20.0',\n",
    "                                     role=role,\n",
    "                                     instance_count=1,\n",
    "                                     instance_type='ml.m5.xlarge')\n",
    "\n",
    "sklearn_processor.run(code='data-preprocessing/data_preprocessing.py',\n",
    "                      arguments = ['--test-ratio', '0.3'],\n",
    "                      inputs=[ProcessingInput(source=input_data,\n",
    "                                              destination='/opt/ml/processing/input')],\n",
    "                      outputs=[ProcessingOutput(destination=train_data,\n",
    "                                                source='/opt/ml/processing/output')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Results of Data Preprocessing\n",
    "Once the preprocessing job is complete, we can take a look at the contents of processing output folder in the S3 bucket to see the transformed data. \n",
    "\n",
    "You should see the following files:\n",
    "\n",
    "* `transient_edges.csv`: the set of edges between each transient uid and each url visited by uid.\n",
    "* `transient_nodes:csv`: the set of transient uid nodes along with the activity feature vector for the uids. \n",
    "* `user_train_edges`: the set of ground-truth same entities for pairs of trainsient uids that will be used during training.\n",
    "* `user_test_edges`: the set of ground-truth same entities that will be used to evaluate the trained model. \n",
    "* `website_group_nodes`: the set of url nodes along with the feature vectors for the urls.\n",
    "* `website_group_edges`: the set of edges between each url and it's parent domain.\n",
    "\n",
    "We add these files to our `param` dictionary because our downstream training job will use these file names to construct the identity graph during model training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "from sagemaker.s3 import S3Downloader\n",
    "processed_files = S3Downloader.list(train_data)\n",
    "print(\"===== Processed Files =====\")\n",
    "print('\\n'.join(processed_files))\n",
    "\n",
    "params = {\n",
    "    'train-edges': 'user_train_edges.csv',\n",
    "    'test-edges': 'user_test_edges.csv',\n",
    "    'transient-nodes': 'transient_nodes.csv',\n",
    "    'transient-edges': 'transient_edges.csv',\n",
    "    'website-nodes': 'website_nodes.csv',\n",
    "    'website-group-edges': 'website_group_edges.csv'\n",
    "}\n",
    "\n",
    "print(\"Graph will be constructed using the following data:\\n{}\".format(params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Graph Neural Network with DGL\n",
    "\n",
    "Graph Neural Networks (GNNs) work by learning numeric representations for nodes and edges informed by the graph structure. We can model the entity resolution problem as a link prediction problems i.e we have a few links between users that are the same entity and we would like to use that information to predict new links/edges between users that are not linked in the graph but may correspond to linked entities.\n",
    "\n",
    "In order to train a model that can achieve this we need to make/specify two modelling assumptions the `GNN Architecture` and the `Self-Supervision Task`.\n",
    "\n",
    "* *GNN Architecture*: The GNN Architecture is what GNN framework is used to learn the node representations that are consumed by downstream task model. Since we have nodes and edges of different types, we will be using a relational graph convolutional neural network model (R-GCN). This architecture using works well on heterogeneous graphs. This is also what is known as the `Graph Encoder`\n",
    "\n",
    "* *Self-Supervision Task*: As alluded to, the task that will be used to supervise the encoder is *link prediciton*. Formally, we generate some negative edges by creating links between nodes sampled at random from the graph. The goal of the task is to learn a score function that gives higher scores to the real positive edges and lower scores to the negative edges. This is what is also known as the `Graph Decoder`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "To train the graph neural network, we need to define a few hyperparameters that determine the GNN architecture, graph sampling parameters, optimizer, and optimization parameters.\n",
    "\n",
    "Here we're setting only a few of the hyperparameters, to see all the hyperparameters and their default values, see dgl-entity-resolution/estimator_fns.py. The parameters set below are:\n",
    "\n",
    "\n",
    "* `mini-batch`: Whether to perform mini-batch training, which training the model with a batch of nodes at a time and using the sampled graph neighbourhood of the mini-batch nodes.\n",
    "* `batch-size`: The number of nodes in a mini-batch that are used to compute a single forward pass of the GNN.\n",
    "* `num-gpus`: The number of gpus to use during training. Use only when training with a GPU enabled instance\n",
    "* `embedding-size`: In the inductive case, the number of dimensions of the node specific embedding that is concatenated with the node feature vector. For nodes that don't have features, the dimensionality is just `embedding-size`.\n",
    "* `n-neighbors`: The number of neighbours to sample for each target node during graph sampling for mini-batch training\n",
    "* `n-layers`: The number of GNN layers in the model\n",
    "* `negative-sampling-rate`: How many `negative edges` to sample from the graph for each positive edge in the mini-batch. This is used to supervise the loss function so it penalize negative edges and distinguishes those from positive edges. \n",
    "* `n-epochs`: The number of training epochs for the model training job. We set this to 3 epochs so that the job terminates quickly. In order to obtain better predictions, train the model for more epochs.\n",
    "* `optimizer`: The optimization algorithm used for gradient based parameter updates\n",
    "* `lr`: The learning rate for parameter updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    'mini-batch': 'true',\n",
    "    'batch-size': 1000,\n",
    "    'num-gpus': 1,\n",
    "    'embedding-size': 64,\n",
    "    'n-neighbors': 100,\n",
    "    'n-hidden': 16,\n",
    "    'n-layers': 2,\n",
    "    'negative-sampling-rate': 10,\n",
    "    'n-epochs': 3,\n",
    "    'optimizer': 'adam',\n",
    "    'lr': 1e-2\n",
    "}\n",
    "\n",
    "params.update(**hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Fit SageMaker Estimator\n",
    "\n",
    "With the hyperparameters defined, we can kick off the training job. We will be using the Deep Graph Library (DGL), with PyTorch as the backend deep learning framework, to define and train the graph neural network. Amazon SageMaker makes it do this with the Framework estimators which have the deep learning frameworks already setup. Here, we create a SageMaker PyTorch estimator and pass in our model training script, hyperparameters, as well as the number and type of training instances we want.\n",
    "\n",
    "We can then fit the estimator on the the training data location in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(entry_point='train_dgl_pytorch_entity_resolution.py',\n",
    "                   source_dir='sagemaker_graph_entity_resolution/dgl_entity_resolution',\n",
    "                   role=role, \n",
    "                   train_instance_count=1,\n",
    "                   train_instance_type='ml.g4dn.xlarge',\n",
    "                   framework_version=\"1.5.0\",\n",
    "                   py_version='py3',\n",
    "                   hyperparameters=params,\n",
    "                   output_path=train_output,\n",
    "                   code_location=train_output,\n",
    "                   sagemaker_session=sagemaker.Session())\n",
    "\n",
    "estimator.fit({'train': train_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the training is completed, the training instances are shut off and SageMaker stores the trained model and new predicted links to the output location in S3."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Data Science JumpStart)",
   "language": "python",
   "name": "HUB_1P_IMAGE"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}